{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirement: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "files need in the data directory:\n",
    "* train_data.csv\n",
    "* test_data.csv\n",
    "* ateco_code_industry.csv\n",
    "* ita_macro_factors.csv\n",
    "* adj_factor.csv\n",
    "* adj_bin.csv\n",
    "* rf_clf.pkl -- pickle file for Pretrained random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions : \n",
    "Please run through all cells before running the notebook. \n",
    "Also, we need original train.csv in the data folder. We are not retraining the data. We have time-dependent features (change in features over 1 year) assuming our test data set is in the sametime frame (or in the futre) then the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pandas import DateOffset\n",
    "\n",
    "data_dir = './data/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions - Not parf ot Harness : Please run below cells first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _calc_fin_ratio(df):\n",
    "    # Leverage Ratio\n",
    "    out_cols =[\"debt_st_to_eqty_tot\", \n",
    "               \"eqty_tot_to_asst_tot\",\n",
    "               \"debt_st_to_deb_tot\",\n",
    "               \"debt_st_to_ebitda\",\n",
    "               \"liq_curr_ratio\",\n",
    "               \"liq_quick_ratio\",\n",
    "               \"wc_net_to_asst_tot\",\n",
    "               \"asst_tang_fixed_to_asst_tot\",\n",
    "               \"asst_intang_fixed_to_asst_tot\",\n",
    "               \"liq_cfo_ratio\",\n",
    "               \"ebitda_to_asst_tot\",\n",
    "               \"prof_operations_to_asst_tot\",\n",
    "               \"ebitda_to_exp_financing\",\n",
    "               \"AR_to_liab_tot\"]\n",
    "    \n",
    "    df[\"debt_st_to_eqty_tot\"] = (df[\"debt_st\"] / df[\"eqty_tot\"]).where(df[\"debt_st\"]!=0, 0)\n",
    "    df[\"eqty_tot_to_asst_tot\"] = df[\"eqty_tot\"] / df[\"asst_tot\"]\n",
    "    df[\"debt_st_to_deb_tot\"] = (df[\"debt_st\"] / (df[\"debt_st\"] + df[\"debt_lt\"])).where(df[\"debt_st\"]!=0, 0)\n",
    "    df[\"debt_st_to_ebitda\"] = (df[\"debt_st\"] / df[\"ebitda\"]).where(df[\"debt_st\"]!=0, 0)\n",
    "    # Liquidity Ratio\n",
    "    df[\"liq_curr_ratio\"] = (df[\"asst_current\"] / df[\"liab_tot\"]).where(df[\"asst_current\"]!=0, 0) # we don't have current asset or liability\n",
    "    df[\"liq_quick_ratio\"] = ((df[\"cash_and_equiv\"] + df[\"AR\"]) / df[\"liab_tot\"]).where((df[\"cash_and_equiv\"] + df[\"AR\"])!=0, 0)\n",
    "    df[\"wc_net_to_asst_tot\"] = df[\"wc_net\"] / df[\"asst_tot\"]\n",
    "    df[\"asst_tang_fixed_to_asst_tot\"] = df[\"asst_tang_fixed\"] / df[\"asst_tot\"]\n",
    "    df[\"asst_intang_fixed_to_asst_tot\"] = df[\"asst_intang_fixed\"] / df[\"asst_tot\"]\n",
    "    df[\"liq_cfo_ratio\"] = (df[\"cf_operations\"] / df[\"liab_tot\"]).where(df[\"cf_operations\"]!=0, 0)\n",
    "    # Profitability Ratio\n",
    "    df[\"ebitda_to_asst_tot\"] = df[\"ebitda\"] / df[\"asst_tot\"]\n",
    "    df[\"prof_operations_to_asst_tot\"] = df[\"prof_operations\"]/ df[\"asst_tot\"]\n",
    "    # Coverage Ratio\n",
    "    df[\"ebitda_to_exp_financing\"] = (df[\"ebitda\"] / df[\"exp_financing\"]).where(df[\"ebitda\"]!=0, 0)\n",
    "    # Activity Ratio\n",
    "    df[\"AR_to_liab_tot\"] = (df[\"AR\"] / df[\"liab_tot\"]).where(df[\"AR\"]!=0, 0) # we don't have current asset or liability\n",
    "    \n",
    "    # now handle +/- inf situations, clip with min/max value excluding inf for each column:\n",
    "    for col in out_cols:\n",
    "        temp_arr = sorted(df[col].unique())\n",
    "        df[col] = np.clip(df[col], a_min=temp_arr[1], a_max=temp_arr[-2])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def _convert_dollars_to_asset_ratio(df, cols):\n",
    "    for col in cols:\n",
    "        if (col + \"_to_asst_tot\") not in df.columns:\n",
    "            df[col + \"_to_asst_tot\"] = df[col] / df[\"asst_tot\"]\n",
    "    df = df.drop(cols, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_adj_factor(result, n_q=50):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    result : pd.DataFrame() - result should have at least two columns : label / proba (proba : predicted probability from the model) \n",
    "    n_q : number of quantiles to use\n",
    "    \n",
    "    Output: \n",
    "    adjustment : pd.DataFrame() - result would have two columns : bin / adj_factor\n",
    "    bin : bins used for qcut\n",
    "    \"\"\"\n",
    "    pred_bin, bins = pd.qcut(result[\"proba\"], q=n_q, labels=50 - np.arange(n_q), retbins=True)\n",
    "    adj_result = result.copy(deep=True)\n",
    "    adj_result[\"bin\"] = pred_bin\n",
    "    \n",
    "    # calc adjustment factor\n",
    "    actual_prob = adj_result.groupby(\"bin\", as_index=False)[\"label\"].mean()\n",
    "    pred_prob = adj_result.groupby(\"bin\", as_index=False)[\"proba\"].mean()\n",
    "    adjustment = actual_prob.merge(pred_prob, on=\"bin\", how=\"left\")\n",
    "    adjustment[\"adj_factor\"] = adjustment[\"label\"] / adjustment[\"proba\"]\n",
    "    \n",
    "    return adjustment[[\"bin\", \"adj_factor\"]], bins\n",
    "\n",
    "def apply_adj_factor(result, adj_factor, bins):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    result : pd.DataFrame() - result should have at least one column : proba\n",
    "    adj_factor, bins : output of calc_adj_factor\n",
    "    \n",
    "    output\n",
    "    result : pd.DataFrame() - now proba is adjusted proba\n",
    "    \"\"\"\n",
    "    \n",
    "    adj_result = result.copy(deep=True)\n",
    "    n_q = len(bins)-1\n",
    "    adj_result[\"bin\"] = pd.cut(adj_result[\"proba\"], bins=bins, labels = n_q - np.arange(n_q), include_lowest=True)\n",
    "    # deal with probability that are outside of the bins (which shouldn't be many..)\n",
    "    adj_result.loc[(adj_result[\"bin\"].isnull()) & (adj_result[\"proba\"] < bins.min()), \"bin\"] = n_q\n",
    "    adj_result.loc[(adj_result[\"bin\"].isnull()) & (adj_result[\"proba\"] > bins.max()), \"bin\"] = 1\n",
    "    \n",
    "    adj_result = adj_result.merge(adj_factor[[\"bin\", \"adj_factor\"]], on=\"bin\", how=\"left\")\n",
    "    adj_result[\"proba\"] = adj_result[\"proba\"] * adj_result[\"adj_factor\"]\n",
    "    \n",
    "    return adj_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Harness Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocessor(df, preproc_params={}, new=True, **kwargs):\n",
    "    \n",
    "    #0. Drop unnecessary columns\n",
    "    for col in df.columns:\n",
    "        if \"Unnamed\" in col:\n",
    "            df = df.drop(col, axis=1)\n",
    "    if \"eqty_corp_family_tot\" in df.columns:\n",
    "        df = df.drop(\"eqty_corp_family_tot\", axis=1)\n",
    "    \n",
    "    #1. Date Formats:\n",
    "    for col in df.columns:\n",
    "        if \"date\" in col:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "    #2. Fill NAs for Default date\n",
    "    df[\"def_date\"] = df[\"def_date\"].fillna(pd.to_datetime(\"2099-12-31\"))\n",
    "    \n",
    "    #3. Generate labels and drop any future-peeking rows\n",
    "    df[\"def_min\"] = df[\"stmt_date\"] + DateOffset(months=3)\n",
    "    df[\"def_max\"] = df[\"stmt_date\"] + DateOffset(months=15)\n",
    "    df[\"def_in_12mo\"] = 0\n",
    "    df[\"def_in_12mo\"] = df[\"def_in_12mo\"].where(\n",
    "        (df[\"def_date\"]<df[\"def_min\"]) | (df[\"def_date\"]>df[\"def_max\"]), 1)\n",
    "    # Future peeking rows from training data\n",
    "    df = df[~(df[\"def_date\"] < df[\"def_min\"])].drop([\"def_min\", \"def_max\"], axis=1)\n",
    "    \n",
    "    #4. Merge ATECO industry sectors: csv file should be in data directory:\n",
    "    df = df.merge(pd.read_csv(data_dir + \"ateco_code_industry.csv\"), on =\"ateco_sector\", how=\"left\").drop(\"ateco_sector\", axis=1)  \n",
    "    # if new, fill na & standardization parameters\n",
    "    dollar_cols = [col for col in df.columns if col not in [\"HQ_city\", \"INDUSTRY\", \"def_date\", \"fs_year\", \"id\", \"legal_struct\",\n",
    "                                                            \"roa\", \"roe\", \"stmt_date\", \"def_in_12mo\", \"asst_tot\", \"days_rec\"]]\n",
    "    num_cols = [\"roa\", \"roe\"]\n",
    "    if new:\n",
    "        mean_df = pd.DataFrame()\n",
    "        for col in dollar_cols:\n",
    "            temp = df.groupby(\"INDUSTRY\", as_index=False).apply(lambda x : (x[col] / x[\"asst_tot\"]).mean())\n",
    "            temp.columns = [\"INDUSTRY\", col]\n",
    "            if mean_df.empty:\n",
    "                mean_df = temp\n",
    "            else:\n",
    "                mean_df = mean_df.merge(temp, on=[\"INDUSTRY\"], how=\"left\")\n",
    "        for col in num_cols:\n",
    "            temp = df.groupby(\"INDUSTRY\", as_index=False)[col].mean()\n",
    "            temp.columns = [\"INDUSTRY\", col]\n",
    "            mean_df = mean_df.merge(temp, on=[\"INDUSTRY\"], how=\"left\")\n",
    "        preproc_params[\"fill_na_mean\"] = mean_df\n",
    "    \n",
    "    mean_df = preproc_params[\"fill_na_mean\"]\n",
    "    \n",
    "    #5. FILL NA with industry specific means:\n",
    "    for col in dollar_cols:\n",
    "        ind_mean = df[[\"INDUSTRY\"]].merge(mean_df[[\"INDUSTRY\", col]], on=[\"INDUSTRY\"], how=\"left\")[col]\n",
    "        df[col] = df[col].where(~df[col].isnull(), ind_mean * df[\"asst_tot\"])\n",
    "    for col in num_cols:\n",
    "        ind_mean = df[[\"INDUSTRY\"]].merge(mean_df[[\"INDUSTRY\", col]], on=[\"INDUSTRY\"], how=\"left\")[col]\n",
    "        df[col] = df[col].where(~df[col].isnull(), ind_mean)\n",
    "    \n",
    "    #6. Calculate all relevant ratios:\n",
    "    df[\"liab_tot\"] = df[\"asst_tot\"] - df[\"eqty_tot\"]\n",
    "    dollar_cols.append(\"liab_tot\")\n",
    "    df = _calc_fin_ratio(df)\n",
    "    df = _convert_dollars_to_asset_ratio(df, dollar_cols)\n",
    "    \n",
    "    #6.1 - drop some factors \n",
    "    # This is from factor selection - to reduce multi-colinearity between factors\n",
    "    factors_to_exclude = [\"AR_to_liab_tot\", \"ebitda_to_asst_tot\", \"goodwill_to_asst_tot\", \"liq_quick_ratio\", \"liq_curr_ratio\", \n",
    "                      \"taxes_to_asst_tot\", \"AR_to_liab_tot\", \"prof_operations_to_asst_tot\", \"liab_tot_to_asst_tot\"]\n",
    "    df = df.drop(factors_to_exclude, axis=1)\n",
    "    if new:\n",
    "        preproc_params[\"train_df\"] = df\n",
    "    \n",
    "    #7. Calculate year-over-year change variables:\n",
    "    if not new:\n",
    "        train_df = preproc_params[\"train_df\"]\n",
    "        train_df[\"usage\"] = \"train\"\n",
    "        df[\"usage\"] = \"test\"\n",
    "        df = pd.concat([train_df, df], sort=False, ignore_index=True)\n",
    "    \n",
    "    chg_cols = [col for col in df.columns if ((col in [\"roe\", \"roa\"]) or (\"_to_\" in col))]\n",
    "    df = df.sort_values([\"id\", \"stmt_date\"])\n",
    "    chg_df = df[[\"id\"] + chg_cols].groupby(\"id\").diff().fillna(0)\n",
    "    chg_df.columns=[c+\"_chg_1y\" for c in chg_df.columns]\n",
    "    df = df.join(chg_df)\n",
    "    \n",
    "    if not new:\n",
    "        df = df[df[\"usage\"]==\"test\"]\n",
    "        df = df.drop(\"usage\", axis=1)\n",
    "    \n",
    "    #8. Merge macro factors:\n",
    "    m_fac = pd.read_csv(data_dir + \"ita_macro_factors.csv\")\n",
    "    df = df.merge(m_fac, on=\"fs_year\", how=\"left\")\n",
    "    \n",
    "    \n",
    "    #8. STANDARDIZE FACTORS\n",
    "    df[\"days_rec\"] = df.days_rec.fillna(0)\n",
    "    no_standardize_cols = ['id', 'stmt_date', 'HQ_city', 'legal_struct', 'ateco_sector','def_date', 'fs_year', \"def_in_12mo\", \"INDUSTRY\"]\n",
    "    if new:\n",
    "        stdz_dict = {}\n",
    "        for col in df.columns:\n",
    "            if col not in no_standardize_cols:\n",
    "                stdz_dict[col] = [df[col].mean(), df[col].std()]\n",
    "        preproc_params[\"stdz_dict\"] = stdz_dict\n",
    "    \n",
    "    stdz_dict = preproc_params[\"stdz_dict\"]\n",
    "    for col in stdz_dict.keys():\n",
    "        df[col] = (df[col] - stdz_dict[col][0]) / stdz_dict[col][1]\n",
    "    \n",
    "    #9. one-hot encoding for INDUSTRY\n",
    "    df = df.join(pd.get_dummies(df[\"INDUSTRY\"]))\n",
    "    df = df.drop([\"id\", \"stmt_date\", \"INDUSTRY\", \"fs_year\", \"legal_struct\", \"HQ_city\", \"def_date\"], axis=1)\n",
    "    \n",
    "    #10. Sort columns to make sure column orders match in the model read back\n",
    "    df = df[sorted(df.columns)]\n",
    "    \n",
    "    \n",
    "    if new:\n",
    "        return df, preproc_params\n",
    "    else:\n",
    "        if kwargs.get(\"final\", True):\n",
    "            if \"def_in_12mo\" in df.columns:\n",
    "                df = df.drop(\"def_in_12mo\", axis=1)\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def estimator(df, fitting_algo, est_params = {}):\n",
    "    \n",
    "    model = fitting_algo(est_params)\n",
    "    \n",
    "    return(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictor(new_df, model):\n",
    "    predictions = model.predict_proba(new_df)[:,1]\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictor_harness(new_df, model, preprocessor, preproc_params = {}, **kwargs):\n",
    "    \n",
    "    print(\"started preprocessing test_data\")\n",
    "    proc_df = preprocessor(new_df, preproc_params, new=False)\n",
    "    print(\"finished preprocessing test_data\")\n",
    "    predictions = predictor(proc_df, model)\n",
    "    \n",
    "    if kwargs.get(\"calibration_params\", None):\n",
    "        print(\"Calibrating Probabililties\")\n",
    "        predictions = pd.DataFrame({\"proba\" : predictions})\n",
    "        adj_fact, bins = kwargs[\"calibration_params\"][\"adj_fact\"], kwargs[\"calibration_params\"][\"bins\"]\n",
    "        predictions = apply_adj_factor(predictions, adj_fact, bins)[\"proba\"]\n",
    "        \n",
    "    # your code here\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this cell (for Prof / TAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started preprocessing test_data\n",
      "finished preprocessing test_data\n",
      "Calibrating Probabililties\n"
     ]
    }
   ],
   "source": [
    "## We need original train.csv in the data folder as well\n",
    "train_data = pd.read_csv(data_dir + \"train_data.csv\")\n",
    "# Please rename test_data.csv to match test file name\n",
    "test_data = pd.read_csv(data_dir + \"test_data.csv\")\n",
    "\n",
    "with open(data_dir+\"rf_clf_fin.pkl\", 'rb') as file:\n",
    "    rf_clf_read = pickle.load(file)\n",
    "\n",
    "preproc_train, params = preprocessor(train_data)\n",
    "\n",
    "adj_factor = pd.read_csv(data_dir+\"adj_fact.csv\")[[\"adj_factor\", \"bin\"]]\n",
    "bins = pd.read_csv(data_dir+\"adj_bin.csv\")[\"bins\"]\n",
    "calibration_params = {\"adj_fact\" : adj_factor, \"bins\": bins}\n",
    "\n",
    "y_pred = predictor_harness(test_data, rf_clf_read, preprocessor, preproc_params=params, calibration_params=calibration_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FBSDE",
   "language": "python",
   "name": "fbsde"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
